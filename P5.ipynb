{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3pG7YkCfLfb",
        "colab_type": "text"
      },
      "source": [
        "# B455: Project 5\n",
        "\n",
        "Serena Patel\n",
        "\n",
        "(04/25/2020)\n",
        "\n",
        "output: pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YA5YbZsfRr-",
        "colab_type": "text"
      },
      "source": [
        "This project is about the classification of movie reviews into positive and negative reviews by training a logistic regression model. We will be using the IMDb dataset. \"The movie review dataset consists of 50,000 popular movie reviews that are labeled as either positive or negative; here, a positive means that a movie was rated with more than six stars on IMDb, and negative means that a movie was rated with fewer than five stars on IMDb.\" The key steps for the training process include: clean text data, process documents into tokens, transform words into feature words, access word relevancy, and build the logistic model. \n",
        "\n",
        "\n",
        "Since the the testing and training data was separated into positive and negative reviews where each folder had each review was in a separate text file. I combined these files into one csv file where it had the review from the data file followed by 1 or 0 depending on if it is positive or negative. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B49EQRNHoWuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('movie_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Ze3sVqotVJ",
        "colab_type": "text"
      },
      "source": [
        "#Transforming Documents Into Feature Vectors\n",
        "\n",
        "By transforming our document into feature vectors, we will create a vocabulary to help us separate, or parse, the data into feature vector. I used sklearn's CountVectorizer feature to implement thi. This simplifies the process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMZGy6s2op4q",
        "colab_type": "code",
        "outputId": "2fa144f0-bc46-40e2-f61c-0ce4d54e08de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = (['The sun is shining',\n",
        "         'The weather is sweet',\n",
        "         'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)\n",
        "\n",
        "print(count.vocabulary_)\n",
        "print(bag.toarray())\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YyGRdAXo1Of",
        "colab_type": "text"
      },
      "source": [
        "#Word Relevancy \n",
        "\n",
        "By using term frequency-inverse document frequency, we can calculate the relevancy of the words. We find this by looking at the number of times a word occurs in relation to the number of words we are analyzing. This allows us to see the words as numbers instead words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QSjpXmOo8Au",
        "colab_type": "code",
        "outputId": "039e4bb0-d587-4e62-fd52-92362467afd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
        "\n",
        "print(tfidf.fit_transform(bag).toarray())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
            " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
            " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8II2wgOo9DD",
        "colab_type": "text"
      },
      "source": [
        "#Data Cleaning\n",
        "\n",
        "Cleaning the data is essential. The essence of the analysis is about words, whether they're negative or positive. So, the emoticons, such as punctuation or other grammatical markings, do not necessarily add much. So, we can consider it to be noise and take it out of the data so we are only looking at words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmmihQoeo-h9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "  text =re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
        "  \n",
        "  return text\n",
        "\n",
        "dataset['review'] = dataset['review'].apply(preprocessor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cpBVEhdpCor",
        "colab_type": "text"
      },
      "source": [
        "#Tokenization\n",
        "\n",
        "\"Tokenization is the process of splitting a string into a list of tokens.\" In particular, we are stemming our words. Stemming essentially takes variations of a word and changes it to its essence. For example, 'watching' and 'watched' turns into 'watch'. We do this because it simplifies our data by taking out the noise from the word. We do not care about if a word is present or past tense, we care about if it has a negative or positive connotation. \n",
        "We also utilize stop words to take out 'useless' words. These are words, such as prepositions or transition words, that add no real value to our analysis of the sentence. \n",
        "\n",
        "We utilize the nltk library for this implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K_CM2ajpD9R",
        "colab_type": "code",
        "outputId": "fefd0174-8ec9-46de-bc02-4e6b9cd7c3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "  return text.split()\n",
        "\n",
        "def tokenizer_stemmer(text):\n",
        "  return[porter.stem(word) for word in text.split()]\n",
        "\n",
        "tokenizer('runners like running thus they run')\n",
        "tokenizer_stemmer('runners like running thus they run')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_stemmer('runners like running thus they run') if w not in stop]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'thu', 'run']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ6BlXN8pRnZ",
        "colab_type": "text"
      },
      "source": [
        "#Transform Data Into Vectors\n",
        "\n",
        "By utilizing the TFID vectorization, we get the frequency of words to understand the importance of the word. The TfidVectorizer \"will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.\" This is a way of understanding how important the words are. This is the final cleaning process before the model is implemented because after cleaning the data as a whole. This goes through and considers the value of the words itself now that we have not only eliminated any data that is of no value and have given the left over words a weight, we can understand the importance of the word as a whole and how it relates to the prediction model. We are left with two feature vectors, y and X which we will utilize in the Linear Regression model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhy0lUxVpT9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                         lowercase=True,\n",
        "                         preprocessor=None,  # applied preprocessor in Data Cleaning\n",
        "                         tokenizer=tokenizer_stemmer,\n",
        "                         use_idf=True,\n",
        "                         norm='l2',\n",
        "                         smooth_idf=True)\n",
        "\n",
        "y = dataset.sentiment.values\n",
        "X = tfidf.fit_transform(dataset.review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJ3fFdnpTeR",
        "colab_type": "text"
      },
      "source": [
        "#Linear Regression\n",
        "\n",
        "Really, this is the implementation of the Linear Regression model with the adjusted dataset. We implement our y and X feature vectors and our target vectors (training and testing data). We will utilize sklearn's LogisticRegressionCV which allows us to compute the linear regression model along with the cross-validation. In this case, we did a 5-fold cross validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxqNZKU2pXZ9",
        "colab_type": "code",
        "outputId": "41a77c7f-cda3-437a-c2a1-91e64620db37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.5, shuffle=False)\n",
        "\n",
        "lrcv = LogisticRegressionCV(cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           random_state=0,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=3,\n",
        "                           max_iter=300).fit(X_train, y_train)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.5min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTU8USFHsCDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_model = open('saved_model.sav', 'wb')\n",
        "pickle.dump(lrcv, saved_model)\n",
        "saved_model.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eq_apumpcbi",
        "colab_type": "text"
      },
      "source": [
        "#Model Evaluation\n",
        "\n",
        "We find our accuracy score for our model to be 89.61%, which isn't too bad. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONIiYducpcIy",
        "colab_type": "code",
        "outputId": "15ebb62e-98f1-4bb4-d257-1bd700dd902c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filename = 'saved_model.sav'\n",
        "saved_lrcv = pickle.load(open(filename, 'rb'))\n",
        "lrcv.score(X_test, y_test)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89608"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKXIokcZtTe_",
        "colab_type": "code",
        "outputId": "7600e85a-5cce-4c3f-9d55-4ffa253ad144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "yhat = lrcv.predict(X_test)\n",
        "yhat"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRRseRSKo3qE",
        "colab_type": "text"
      },
      "source": [
        "#Resources\n",
        "\n",
        "\n",
        "\n",
        "*   [Sentiment Analysis With Python](https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a)\n",
        "*   [How to Clean Text for Machine Learning with Python](https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
        "*   [How to Tokenize Tweets with Python](https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7)\n",
        "*   [Building Blocks: Text Pre-Processing](https://towardsdatascience.com/building-blocks-text-pre-processing-641cae8ba3bf)\n",
        "*   [Natural Language Processing: Text Data Vectorization](https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7)\n",
        "*   [How to Prepare Text Data for Machine Learning](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
        "\n",
        "\n",
        "Dataset from [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n"
      ]
    }
  ]
}